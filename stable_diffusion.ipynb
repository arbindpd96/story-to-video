{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stable-diffusion.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet --upgrade diffusers transformers accelerate mediapy peft openai gtts pyttsx3 ffmpeg-python"
      ],
      "metadata": {
        "id": "ufD_d64nr08H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "script = \"\"\"scene_heading:INT. HOUSE (ENTRY LOBBY) -DAY\"\n",
        "action:A ghoulish figure is standing on stairs going to the basement looking towards the protagonist.\n",
        "action:The wall above the ghoulish figure reads 'THIS PERSON IS NOT A TENANT IN THIS BUILDING'\n",
        "action:Camera pans to the protagonist, standing by the door of the attached room, intially staring at the ghoulish figure and then slightly glancing at the text above.\n",
        "action:Camera pans back to the ghoulish figure, the text on the wall has changed to 'THEY JUST KEEP STARING AT ME'\n",
        "action:Camera pans back to the protagonist, he closes the door.\n",
        "scene_heading:INT. HOUSE (ATTACHED ROOM) -DAY\n",
        "action:The protagonist closes the door and looks through the peep hole and walks off.\n",
        "scene_heading:INT. HOUSE (KITCHEN) -DAY\n",
        "action:The protagonist walks through the kitchen to the another room.\n",
        "scene_heading:INT. HOUSE (LIVING ROOM) -DAY\n",
        "action:The protagonist walks towards the phone kept on the table, picks it up and heads back to the kitchen.\n",
        "scene_heading:INT. HOUSE (KITCHEN) -DAY\n",
        "action:The protagonist is calling somebody on the phone and looks towards his left.\n",
        "action:Camera pans, the ghoulish figure is standing in the kitchen and staring at the protagonist.\n",
        "action:The protagonist is holding the phone to his ear and staring at the ghoulish figure.\n",
        "action:The ghoulish figure is staring at the protagonist,standing still.\n",
        "action:The protagonist is still holding the phone to his ear, staring at the ghoulish figure and starts moving backwards to the living room.\n",
        "scene_heading:INT. HOUSE (LIVING ROOM) -DAY\n",
        "action:The protagonist is moving backwards holding the phone to his ear, moving backwards. he drops the phone on the floor.\n",
        "action:The phone has fallen on the floor, and protagonist is walking fast towards another room.\n",
        "scene_heading:INT. HOUSE (BEDROOM) -DAY\n",
        "action:The protagonist is taking something out of his cupboard.\n",
        "action:The protagonist has taken out a bag from the cupboard.\n",
        "action:The bag is thrown on the floor and the protagonist takes out a hammer from the bag.\n",
        "scene_heading:INT. HOUSE (LIVING ROOM) -DAY\n",
        "action:The protagonist is holding the hammer in his hand and looking towards the other the kitchen.\n",
        "action:POV of the protagonist looking towards the empty kitchen.\n",
        "action:Camera pans to the protagonist still holding the hammer in his hand, slowly walking in the living room.\n",
        "action:POV of the protagonist, camera pans to left of the protgonist in the living room.\n",
        "action:The protagonist is looking towards his left with hammer still in the hand, the ghoulish figure's head is seen above the protagonist also peeking towards the same direction as him.\n",
        "action:END CREDIT - BLACK SCREEN - 'TENANT' written on screen\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TvZot2C6HCnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call gpt and pass the script as context and ask for the director shots which will be needed to create an Image."
      ],
      "metadata": {
        "id": "UBOrSEqHGqYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ['OPENAI_API_KEY'],\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"\"\"provided with a plot, generate a Shot script which includes shot description given by a the Director which should be in a format of a prompt to generate its equivalent shot image(should be below 77 tokens) which should look exactly like what a director would have imagined in his brain. Strictly respond in below format\n",
        "                        [\n",
        "                          {\n",
        "                            \"shot\" : <Shot description>,\n",
        "                            \"image_prompt\" : <Shot image prompt representing the entire shot precisely like a movie scene>\n",
        "                          }\n",
        "                        ]\n",
        "                        make atleast 10-12 shots with each shot if shown for 10secounds should combined and make a video of 2mins(120secs)\n",
        "                      \"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": script,\n",
        "            \"response_format\" : {\"type\": \"json_object\"}\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4-turbo\",\n",
        ")\n",
        "shots = chat_completion.choices[0].message.content\n",
        "print(shots)"
      ],
      "metadata": {
        "id": "---Qtd5JFbox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapy as media\n",
        "import random\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "from diffusers import DiffusionPipeline, TCDScheduler\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Choose among 1, 2, 4 and 8:\n",
        "num_inference_steps = 8\n",
        "\n",
        "base_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "repo_name = \"ByteDance/Hyper-SD\"\n",
        "plural = \"s\" if num_inference_steps > 1 else \"\"\n",
        "ckpt_name = f\"Hyper-SDXL-{num_inference_steps}step{plural}-lora.safetensors\"\n",
        "device = \"cuda\"\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n",
        "pipe.load_lora_weights(hf_hub_download(repo_name, ckpt_name))\n",
        "pipe.fuse_lora()\n",
        "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)"
      ],
      "metadata": {
        "id": "bG2hkmSEvByV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = random.randint(0, sys.maxsize)"
      ],
      "metadata": {
        "id": "75bZtxWhZEIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "output_folder = \"output2\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "shots_list = json.loads(shots)\n",
        "# Iterate over each object in the list\n",
        "for i, shot in enumerate(shots_list):\n",
        "    # Extract the image prompt from the shot\n",
        "    prompt = shot[\"image_prompt\"]\n",
        "\n",
        "    # Set the eta value (adjust as needed)\n",
        "    eta = 0.9\n",
        "\n",
        "    # Generate the image\n",
        "    images = pipe(\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=0.9,\n",
        "        eta=eta,\n",
        "        generator=torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "    ).images\n",
        "\n",
        "    # Print the prompt and seed\n",
        "    print(f\"Prompt:\\t{prompt}\\nSeed:\\t{seed}\")\n",
        "\n",
        "    # Save the generated image\n",
        "    output_path = os.path.join(output_folder, f\"output_{i}.jpg\")\n",
        "    images[0].save(output_path)"
      ],
      "metadata": {
        "id": "DsrR2VasBjqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this for video generation from all the images that was generated. with audio in it"
      ],
      "metadata": {
        "id": "bOvZmXHIMObV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from moviepy.editor import ImageClip, concatenate_videoclips\n",
        "from moviepy.video.fx.all import fadein, fadeout\n",
        "from gtts import gTTS\n",
        "import ffmpeg\n",
        "\n",
        "# Set the input and output paths\n",
        "input_folder = \"output2\"\n",
        "output_video = \"video.mp4\"\n",
        "audio_file = \"narration.mp3\"\n",
        "\n",
        "# Set the desired duration for each image (in seconds)\n",
        "duration = 6\n",
        "\n",
        "# Get the list of image files in the input folder\n",
        "image_files = [f for f in os.listdir(input_folder) if f.endswith(\".jpg\")]\n",
        "\n",
        "# Sort the image files based on their names\n",
        "image_files.sort()\n",
        "\n",
        "# Create a list of image clips with fade-in and fade-out transitions\n",
        "clips = []\n",
        "for image_file in image_files:\n",
        "    clip = ImageClip(os.path.join(input_folder, image_file)).set_duration(duration)\n",
        "    clip = fadein(clip, 1)  # Fade-in duration of 1 second\n",
        "    clip = fadeout(clip, 1)  # Fade-out duration of 1 second\n",
        "    clips.append(clip)\n",
        "\n",
        "# Concatenate the image clips\n",
        "video_clip = concatenate_videoclips(clips)\n",
        "\n",
        "# Set the text for narration\n",
        "narration_text = \"\"\"In this chilling tale, a protagonist finds himself in a haunting encounter with a ghoulish figure in his house. The eerie figure, standing on the basement stairs, bears a message above its head: \"THIS PERSON IS NOT A TENANT IN THIS BUILDING.\" As the protagonist's gaze shifts between the figure and the message, the text mysteriously changes to \"THEY JUST KEEP STARING AT ME,\" intensifying the unnerving atmosphere.\n",
        "Seeking refuge, the protagonist closes the door and cautiously peeks through the peephole before moving through the kitchen and into the living room. With trembling hands, he picks up the phone, attempting to call for help. However, as he returns to the kitchen, he is confronted by the ghoulish figure, now standing motionless and staring directly at him.\n",
        "Gripped by fear, the protagonist retreats to the living room, his phone falling to the floor as he hastily makes his way to the bedroom. Desperate for protection, he rummages through his cupboard, retrieving a bag containing a hammer. Armed with the tool, he ventures back into the living room, scanning the now empty kitchen with apprehension.\n",
        "As the protagonist slowly moves through the living room, a chilling revelation unfolds. The ghoulish figure's head emerges above him, peeking in the same direction, as if mirroring his every move. The story concludes with a black screen and the ominous word \"TENANT,\" leaving the viewer with a lingering sense of unease and the realization that the true nature of the protagonist's haunting experience remains a mystery.\"\"\"\n",
        "\n",
        "\n",
        "# Generate audio from the text using gTTS\n",
        "tts = gTTS(text=narration_text, lang='en')\n",
        "tts.save(audio_file)\n",
        "\n",
        "# Write the video clip to a temporary file\n",
        "video_clip.write_videofile(\"temp_video.mp4\", fps=24, codec='libx264')\n",
        "\n",
        "# Use ffmpeg-python to combine the video and audio files\n",
        "input_video = ffmpeg.input(\"temp_video.mp4\")\n",
        "input_audio = ffmpeg.input(audio_file)\n",
        "output = ffmpeg.output(input_video, input_audio, output_video, vcodec='copy', acodec='aac', strict='experimental')\n",
        "ffmpeg.run(output)\n",
        "\n",
        "# Remove the temporary files\n",
        "os.remove(\"temp_video.mp4\")\n",
        "os.remove(audio_file)"
      ],
      "metadata": {
        "id": "pFTOa_rKMGfP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}